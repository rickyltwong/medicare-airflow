AIRFLOW_UID=1000

# alternative way to add connections (in this project, we use airflow CLI as this is easier to test and debug)
AIRFLOW_CONN_DO_SPACES=aws://<access-key>:<secret-key>@<endpoint>/<bucket>?endpoint_url=<endpoint_url>?region_name=<region_name>
AIRFLOW_CONN_SNOWFLAKE=snowflake://<username>:<password>@/<schema>?account=<account>&database=<database>&region=<region>&warehouse=<warehouse>


# AIRFLOW_CONN_SPARK=spark://<ip>:<port>/?deploy-mode-client&spark-binary=spark-submit # it doesn't work for some reasons, neither does the following

# [2025-02-21, 21:57:19 UTC] {spark_submit.py:474} INFO - Spark-Submit cmd: spark-submit --master standalone-spark-master-1:7077 --conf spark.driver.bindAddress=0.0.0.0 --name arrow-spark /opt/***/dags/scripts/wordcountjob.py
# [2025-02-21, 21:57:19 UTC] {spark_submit.py:641} INFO - Picked up _JAVA_OPTIONS: -Djava.net.preferIPv4Stack=true
# [2025-02-21, 21:57:20 UTC] {spark_submit.py:641} INFO - Picked up _JAVA_OPTIONS: -Djava.net.preferIPv4Stack=true
# [2025-02-21, 21:57:22 UTC] {spark_submit.py:641} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
# [2025-02-21, 21:57:22 UTC] {spark_submit.py:641} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:1052)

# AIRFLOW_CONN_SPARK=spark://spark://<ip>:<port>/?deploy-mode-client&spark-binary=spark-submit # as the system could not parse the url

# airflow.exceptions.AirflowException: Invalid connection string: spark://spark://standalone-spark-master-1:7077/?deploy-mode-client&spark-binary=spark-submit.

# So, instead of createing the connection using environment variable, we can create this connection using the CLI
# airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'spark://standalone-spark-master-1' --conn-port '7077'
